{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_map_db15k = {'birthDate':'birthDate',\n",
    " 'completionDate': 'completionDate',\n",
    " 'deathDate':'deathDate',\n",
    " 'formationDate':'formationDate',\n",
    " 'foundingDate': 'foundingDate',\n",
    " 'height': 'height',\n",
    " 'releaseDate': 'releaseDate',\n",
    " 'wgs84_pos#lat': 'latitude',\n",
    " 'wgs84_pos#long': 'longitude'}\n",
    "rel_map_mutag = {\n",
    "    'http://dl-learner.org/mutagenesis#act' : 'mutagenesis\\#act',\n",
    " 'http://dl-learner.org/mutagenesis#charge': 'mutagenesis\\#charge',\n",
    " 'http://dl-learner.org/mutagenesis#logp' : 'mutagenesis\\#logp',\n",
    " 'http://dl-learner.org/mutagenesis#lumo' : 'mutagenesis\\#lumo'\n",
    "}\n",
    "rel_map_fb15k = {\n",
    "    'people.person.date_of_birth': 'date\\_of\\_birth',\n",
    "    'people.deceased_person.date_of_death': 'date\\_of\\_death',\n",
    "    'film.film.initial_release_date': 'release\\_date',\n",
    "    'organization.organization.date_founded': 'org.date\\_founded',\n",
    "    'location.dated_location.date_founded': 'loc.date\\_founded',\n",
    "    'location.geocode.latitude': 'latitude',\n",
    "    'location.geocode.longitude': 'longitude',\n",
    "    'location.location.area': 'loc.area',\n",
    "    'topic_server.population_number': 'pop.\\_number',\n",
    "    'people.person.height_meters': 'height\\_meters',\n",
    "    'people.person.weight_kg': 'weight\\_kg'\n",
    "}\n",
    "rel_map_yago15k = {\n",
    "    'diedOnDate': 'diedOnDate',\n",
    "    'happenedOnDate': 'happenedOnDate',\n",
    "    'hasLatitude': 'Latitude',\n",
    "    'hasLongitude': 'Longitude',\n",
    "    'wasBornOnDate': 'BornOnDate',\n",
    "    'wasCreatedOnDate': 'CreatedOnDate',\n",
    "    'wasDestroyedOnDate': 'DestroyedOnDate'\n",
    "}\n",
    "rel_map_yago15k_short = {\n",
    "    'diedOnDate': 'diedOnDate',\n",
    "    'happenedOnDate': 'happenedOnDate',\n",
    "    'hasLatitude': 'Latitude',\n",
    "    'hasLongitude': 'Longitude',\n",
    "    'wasBornOnDate': 'BornOnDate',\n",
    "    'wasCreatedOnDate': 'CreatedOn',\n",
    "    'wasDestroyedOnDate': 'DestroyedOn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "relation & MAE_GLOBAL & MAE_LOCAL & LR_MAE & PLM_reg:MAE & MAE_litem \\\\\n",
      "\\midrule\n",
      "release\\_date & 12.200 & 9.849 & 5.590 & 5.526 & 4.258 \\\\\n",
      "loc.date\\_founded & 220.980 & 219.205 & 145.460 & 153.387 & 150.873 \\\\\n",
      "latitude & 12.403 & 2.997 & 8.470 & 8.741 & 5.221 \\\\\n",
      "longitude & 58.391 & 7.832 & 25.560 & 24.959 & 11.325 \\\\\n",
      "loc.area & 1842883.439 & 5101534.591 & 770000.000 & 1778305.891 & 1652001.300 \\\\\n",
      "org.date\\_founded & 81.029 & 81.482 & 53.850 & 71.632 & 46.741 \\\\\n",
      "date\\_of\\_death & 56.561 & 48.505 & 35.890 & 45.575 & 32.620 \\\\\n",
      "date\\_of\\_birth & 35.802 & 25.187 & 26.600 & 23.489 & 18.824 \\\\\n",
      "height\\_meters & 0.079 & 0.088 & 0.065 & 0.162 & 0.074 \\\\\n",
      "weight\\_kg & 12.506 & 16.367 & NaN & 9.515 & 12.653 \\\\\n",
      "pop.\\_number & 7184317.390 & 137764431.914 & 7900000.000 & 25998727.367 & 4873297.417 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 1 : Compare LitEm with GOBAL LOCAL LR KGE_Reg FB15k-237\n",
    "import pandas as pd\n",
    "dataset = \"FB15k-237\"\n",
    "local_global_df =  pd.read_csv(f\"Stats/{dataset}_LOCAL_GLOBAL.csv\", sep=',')\n",
    "approaches_df = pd.read_csv(f\"Stats/{dataset}_Reg.csv\", sep=',')\n",
    "litem_df = pd.read_csv(f\"Experiments/Literals/{dataset}/TransE_100/lit_results.csv\", sep=',')\n",
    "mae_cols = [col for col in litem_df.columns if \"MAE\" in col]\n",
    "rmse_cols = [col for col in litem_df.columns if \"RMSE\" in col]\n",
    "\n",
    "# Compute the mean across runs\n",
    "litem_df[\"MAE_litem\"] = litem_df[mae_cols].mean(axis=1)\n",
    "litem_df[\"RMSE_litem\"] = litem_df[rmse_cols].mean(axis=1)\n",
    "litem_dfs = litem_df[[\"relation\",\"MAE_litem\",\"RMSE_litem\"]]\n",
    "merged_df = pd.merge( local_global_df, approaches_df, on='relation', how='inner') \n",
    "final_df = pd.merge(merged_df, litem_dfs, on='relation', how='inner')\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "filtered_df.loc[:, \"relation\"] = filtered_df.loc[:, \"relation\"].map(rel_map_fb15k)\n",
    "latex_table = filtered_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "relation & MAE_GLOBAL & MAE_LOCAL & LR_MAE & MAE_PLM_reg & MAE_litem \\\\\n",
      "\\midrule\n",
      "diedOnDate & 58.508 & 55.072 & NaN & 50.119 & 40.515 \\\\\n",
      "happenedOnDate & 54.444 & 54.444 & NaN & 96.691 & 26.064 \\\\\n",
      "Latitude & 9.170 & 2.739 & NaN & 8.656 & 3.090 \\\\\n",
      "Longitude & 60.466 & 9.914 & NaN & 25.907 & 11.506 \\\\\n",
      "BornOnDate & 26.495 & 24.100 & NaN & 19.795 & 15.586 \\\\\n",
      "CreatedOnDate & 91.565 & 148.035 & NaN & 70.586 & 57.081 \\\\\n",
      "DestroyedOnDate & 42.639 & 43.554 & NaN & 41.972 & 21.436 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 1 : Compare LitEm with GOBAL LOCAL LR KGE_Reg-YAGO15k\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"YAGO15k\"\n",
    "local_global_df =  pd.read_csv(f\"Stats/{dataset}_LOCAL_GLOBAL.csv\", sep=',')\n",
    "approaches_df = pd.read_csv(f\"Stats/{dataset}_Reg.csv\", sep=',')\n",
    "litem_df = pd.read_csv(f\"Experiments/Literals/{dataset}/TransE_100/lit_results.csv\", sep=',')\n",
    "mae_cols = [col for col in litem_df.columns if \"MAE\" in col]\n",
    "rmse_cols = [col for col in litem_df.columns if \"RMSE\" in col]\n",
    "\n",
    "# Compute the mean across runs\n",
    "litem_df[\"MAE_litem\"] = litem_df[mae_cols].mean(axis=1)\n",
    "litem_df[\"RMSE_litem\"] = litem_df[rmse_cols].mean(axis=1)\n",
    "litem_dfs = litem_df[[\"relation\",\"MAE_litem\",\"RMSE_litem\"]]\n",
    "merged_df = pd.merge( local_global_df, approaches_df, on='relation', how='inner') \n",
    "final_df = pd.merge(merged_df, litem_dfs, on='relation', how='inner')\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "filtered_df.loc[:, \"relation\"] = filtered_df.loc[:, \"relation\"].map(rel_map_yago15k)\n",
    "latex_table = filtered_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "relation & KGA_MAE & NAP++_MAE & MrAP_MAE & MAE_litem \\\\\n",
      "\\midrule\n",
      "date\\_of\\_birth & 18.900 & 22.100 & 15.000 & 18.824 \\\\\n",
      "date\\_of\\_death & 20.600 & 52.300 & 16.300 & 32.620 \\\\\n",
      "release\\_date & 4.000 & 9.900 & 6.300 & 4.258 \\\\\n",
      "org.date\\_founded & 49.000 & 59.300 & 58.300 & 46.741 \\\\\n",
      "loc.date\\_founded & 76.000 & 92.100 & 98.800 & 150.873 \\\\\n",
      "latitude & 2.100 & 11.800 & 1.500 & 5.221 \\\\\n",
      "longitude & 7.100 & 54.700 & 4.000 & 11.325 \\\\\n",
      "loc.area & 61000.000 & 440000.000 & 440000.000 & 1652001.300 \\\\\n",
      "pop.\\_number & 4000000.000 & 7500000.000 & 21000000.000 & 4873297.417 \\\\\n",
      "height\\_meters & 0.077 & 0.080 & 0.086 & 0.074 \\\\\n",
      "weight\\_kg & 11.600 & 15.300 & 12.900 & 12.653 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# table hidden : Compare LitEm with KGA, NAP++, FB15k-237\n",
    "import pandas as pd\n",
    "dataset = \"FB15k-237\"\n",
    "sota_df =  pd.read_csv(f\"Stats/SOTA_LitPreds_{dataset}.csv\", sep=',')\n",
    "litem_df = pd.read_csv(f\"Experiments/Literals/{dataset}/TransE_100/lit_results.csv\", sep=',')\n",
    "mae_cols = [col for col in litem_df.columns if \"MAE\" in col]\n",
    "rmse_cols = [col for col in litem_df.columns if \"RMSE\" in col]\n",
    "# Compute the mean across runs\n",
    "litem_df[\"MAE_litem\"] = litem_df[mae_cols].mean(axis=1)\n",
    "litem_df[\"RMSE_litem\"] = litem_df[rmse_cols].mean(axis=1)\n",
    "litem_dfs = litem_df[[\"relation\",\"MAE_litem\",\"RMSE_litem\"]]\n",
    "final_df = pd.merge(sota_df, litem_dfs, on='relation', how='inner')\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "filtered_df.loc[:, \"relation\"] = filtered_df.loc[:, \"relation\"].map(rel_map_fb15k)\n",
    "latex_table = filtered_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "relation & KGA_MAE & NAP++_MAE & MrAP_MAE & MAE_litem \\\\\n",
      "\\midrule\n",
      "BornOnDate & 16.300 & 23.200 & 19.700 & 15.586 \\\\\n",
      "diedOnDate & 30.800 & 45.700 & 34.000 & 40.515 \\\\\n",
      "CreatedOnDate & 58.200 & 83.500 & 70.400 & 57.081 \\\\\n",
      "DestroyedOnDate & 23.300 & 38.200 & 34.600 & 21.436 \\\\\n",
      "happenedOnDate & 29.900 & 73.700 & 54.100 & 26.064 \\\\\n",
      "Latitude & 3.400 & 8.700 & 2.800 & 3.090 \\\\\n",
      "Longitude & 7.200 & 43.100 & 5.700 & 11.506 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# table hidden : Compare LitEm with KGA, NAP++, YAGO\n",
    "import pandas as pd\n",
    "dataset = \"YAGO15k\"\n",
    "sota_df =  pd.read_csv(f\"Stats/SOTA_LitPreds_{dataset}.csv\", sep=',')\n",
    "litem_df = pd.read_csv(f\"Experiments/Literals/{dataset}/TransE_100/lit_results.csv\", sep=',')\n",
    "mae_cols = [col for col in litem_df.columns if \"MAE\" in col]\n",
    "rmse_cols = [col for col in litem_df.columns if \"RMSE\" in col]\n",
    "# Compute the mean across runs\n",
    "litem_df[\"MAE_litem\"] = litem_df[mae_cols].mean(axis=1)\n",
    "litem_df[\"RMSE_litem\"] = litem_df[rmse_cols].mean(axis=1)\n",
    "litem_dfs = litem_df[[\"relation\",\"MAE_litem\",\"RMSE_litem\"]]\n",
    "final_df = pd.merge(sota_df, litem_dfs, on='relation', how='inner')\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "filtered_df.loc[:, \"relation\"] = filtered_df.loc[:, \"relation\"].map(rel_map_yago15k)\n",
    "latex_table = filtered_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "relation & MAE_GLOBAL & RMSE_GLOBAL & MAE_LOCAL & RMSE_LOCAL & MAE_litem & RMSE_litem \\\\\n",
      "\\midrule\n",
      "birthDate & 23.361 & 34.421 & 21.695 & 31.137 & 12.408 & 19.697 \\\\\n",
      "completionDate & 9.425 & 11.298 & 7.692 & 9.485 & 4.735 & 5.880 \\\\\n",
      "deathDate & 26.636 & 38.643 & 25.987 & 36.824 & 17.624 & 32.317 \\\\\n",
      "formationDate & 40.358 & 47.105 & 40.358 & 47.105 & 46.091 & 53.207 \\\\\n",
      "foundingDate & 52.944 & 63.124 & 52.407 & 69.379 & 35.324 & 48.632 \\\\\n",
      "height & 1.403 & 1.408 & 1.403 & 1.408 & 2.060 & 2.505 \\\\\n",
      "releaseDate & 13.332 & 18.405 & 11.583 & 17.082 & 9.866 & 15.684 \\\\\n",
      "latitude & 11.830 & 17.656 & 7.081 & 15.124 & 4.811 & 9.161 \\\\\n",
      "longitude & 59.821 & 69.813 & 29.056 & 48.339 & 16.153 & 30.937 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 3: Part 1 DB15k GLOBAL LOCAL LitEM\n",
    "import pandas as pd\n",
    "dataset = \"DB15K\"\n",
    "local_global_df =  pd.read_csv(f\"Stats/{dataset}_LOCAL_GLOBAL.csv\", sep=',')\n",
    "# approaches_df = pd.read_csv(f\"Stats/{dataset}_Reg.csv\", sep=',')\n",
    "litem_df = pd.read_csv(f\"Experiments/Literals/{dataset}/TransE_100/lit_results.csv\", sep=',')\n",
    "mae_cols = [col for col in litem_df.columns if \"MAE\" in col]\n",
    "rmse_cols = [col for col in litem_df.columns if \"RMSE\" in col]\n",
    "\n",
    "# Compute the mean across runs\n",
    "litem_df[\"MAE_litem\"] = litem_df[mae_cols].mean(axis=1)\n",
    "litem_df[\"RMSE_litem\"] = litem_df[rmse_cols].mean(axis=1)\n",
    "litem_dfs = litem_df[[\"relation\",\"MAE_litem\",\"RMSE_litem\"]]\n",
    "# merged_df = pd.merge( local_global_df, approaches_df, on='relation', how='inner') \n",
    "final_df = pd.merge(local_global_df, litem_dfs, on='relation', how='inner')\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "final_df.loc[:, \"relation\"] = final_df.loc[:, \"relation\"].map(rel_map_db15k)\n",
    "latex_table = final_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "relation & MAE_GLOBAL & RMSE_GLOBAL & MAE_LOCAL & RMSE_LOCAL & MAE_litem & RMSE_litem \\\\\n",
      "\\midrule\n",
      "http://dl-learner.org/mutagenesis#act & 1.932 & 2.301 & 1.932 & 2.301 & 1.467 & 1.827 \\\\\n",
      "http://dl-learner.org/mutagenesis#charge & 0.195 & 0.277 & 0.195 & 0.277 & 0.085 & 0.146 \\\\\n",
      "http://dl-learner.org/mutagenesis#logp & 1.218 & 1.569 & 1.218 & 1.569 & 0.739 & 0.965 \\\\\n",
      "http://dl-learner.org/mutagenesis#lumo & 0.361 & 0.451 & 0.361 & 0.451 & 0.302 & 0.366 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 3: Part 2: Mutag GLOBAL LOCAL LitEM\n",
    "import pandas as pd\n",
    "dataset = \"mutagenesis\"\n",
    "local_global_df =  pd.read_csv(f\"Stats/{dataset}_LOCAL_GLOBAL.csv\", sep=',')\n",
    "# approaches_df = pd.read_csv(f\"Stats/{dataset}_Reg.csv\", sep=',')\n",
    "litem_df = pd.read_csv(f\"Experiments/Literals/{dataset}/TransE_100/lit_results.csv\", sep=',')\n",
    "mae_cols = [col for col in litem_df.columns if \"MAE\" in col]\n",
    "rmse_cols = [col for col in litem_df.columns if \"RMSE\" in col]\n",
    "\n",
    "# Compute the mean across runs\n",
    "litem_df[\"MAE_litem\"] = litem_df[mae_cols].mean(axis=1)\n",
    "litem_df[\"RMSE_litem\"] = litem_df[rmse_cols].mean(axis=1)\n",
    "litem_dfs = litem_df[[\"relation\",\"MAE_litem\",\"RMSE_litem\"]]\n",
    "# merged_df = pd.merge( local_global_df, approaches_df, on='relation', how='inner') \n",
    "final_df = pd.merge(local_global_df, litem_dfs, on='relation', how='inner')\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "filtered_df.loc[:, \"relation\"] = filtered_df.loc[:, \"relation\"].map(rel_map_mutag)\n",
    "# filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "latex_table = final_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def get_kge_combined_table(dataset : str):  # Define column names\n",
    "    columns = [\"Model\", \"Dataset\", \"MRR\", \"H@1\", \"H@3\", \"H@10\", \n",
    "               \"MRR_Combined\", \"H@1_Combined\", \"H@3_Combined\", \"H@10_Combined\"]\n",
    "    \n",
    "    # Initialize an empty list to store data\n",
    "    data_rows = []\n",
    "    \n",
    "    exp_models = [\"ComplEx\", \"DeCaL\", \"DistMult\", \"Keci\", \"OMult\", \"QMult\", \"TransE\", \"Pykeen_MuRE\"]\n",
    "    \n",
    " \n",
    "    for model in exp_models:\n",
    "        # Define paths for original and combined evaluation reports\n",
    "        eval_file = f\"Experiments/KGE_old/{dataset}/{model}/eval_report.json\"\n",
    "        combined_dir = f\"Experiments/KGE_Combined_old/{dataset}_old/{model}/eval_report.json\"\n",
    "    \n",
    "        # Check if files exist before opening them\n",
    "        if not os.path.exists(eval_file) or not os.path.exists(combined_dir):\n",
    "            print(f\"Skipping {model} on {dataset} - Missing files\")\n",
    "            continue\n",
    "        \n",
    "        # Load original evaluation data\n",
    "        with open(eval_file, 'r') as file:\n",
    "            eval_data = json.load(file)\n",
    "    \n",
    "        # Load combined evaluation data\n",
    "        with open(combined_dir, 'r') as file:\n",
    "            eval_data_combined = json.load(file)\n",
    "    \n",
    "        # Function to extract metrics\n",
    "        def extract_metrics(data, split_name):\n",
    "            metrics = data.get(split_name, {})\n",
    "            return [\n",
    "                metrics.get(\"MRR\", \"\"), metrics.get(\"H@1\", \"\"), \n",
    "                metrics.get(\"H@3\", \"\"), metrics.get(\"H@10\", \"\")\n",
    "            ]\n",
    "    \n",
    "        # Iteratively add rows for Train, Test, and Val datasets\n",
    "        for split in [\"Train\", \"Test\", \"Val\"]:  # Renamed loop variable to avoid conflict\n",
    "            row = [model, dataset] + extract_metrics(eval_data, split) + extract_metrics(eval_data_combined, split)\n",
    "            data_rows.append(row)  # Append instead of overwriting\n",
    "    \n",
    "    # Create DataFrame from collected rows\n",
    "    df = pd.DataFrame(data_rows, columns=columns)\n",
    "    sub_df = df.drop(columns=['Dataset'])\n",
    "    return sub_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "Model & MRR & H@1 & H@3 & H@10 & MRR_Combined & H@1_Combined & H@3_Combined & H@10_Combined \\\\\n",
      "\\midrule\n",
      "ComplEx & 0.891 & 0.849 & 0.924 & 0.961 & 0.632 & 0.543 & 0.680 & 0.797 \\\\\n",
      "ComplEx & 0.302 & 0.227 & 0.331 & 0.449 & 0.194 & 0.127 & 0.212 & 0.328 \\\\\n",
      "ComplEx & 0.306 & 0.233 & 0.337 & 0.447 & 0.197 & 0.129 & 0.217 & 0.329 \\\\\n",
      "DeCaL & 0.887 & 0.849 & 0.914 & 0.955 & 0.870 & 0.822 & 0.906 & 0.954 \\\\\n",
      "DeCaL & 0.301 & 0.228 & 0.331 & 0.439 & 0.307 & 0.234 & 0.336 & 0.450 \\\\\n",
      "DeCaL & 0.307 & 0.236 & 0.336 & 0.444 & 0.310 & 0.239 & 0.338 & 0.449 \\\\\n",
      "DistMult & 0.860 & 0.810 & 0.898 & 0.946 & 0.873 & 0.828 & 0.904 & 0.952 \\\\\n",
      "DistMult & 0.283 & 0.209 & 0.312 & 0.426 & 0.282 & 0.208 & 0.310 & 0.430 \\\\\n",
      "DistMult & 0.288 & 0.216 & 0.317 & 0.432 & 0.287 & 0.213 & 0.316 & 0.434 \\\\\n",
      "Keci & 0.825 & 0.769 & 0.862 & 0.922 & 0.849 & 0.793 & 0.890 & 0.943 \\\\\n",
      "Keci & 0.259 & 0.186 & 0.286 & 0.401 & 0.306 & 0.234 & 0.335 & 0.447 \\\\\n",
      "Keci & 0.258 & 0.186 & 0.285 & 0.402 & 0.311 & 0.241 & 0.338 & 0.449 \\\\\n",
      "OMult & 0.766 & 0.693 & 0.814 & 0.894 & 0.813 & 0.753 & 0.851 & 0.921 \\\\\n",
      "OMult & 0.242 & 0.171 & 0.269 & 0.376 & 0.250 & 0.175 & 0.280 & 0.393 \\\\\n",
      "OMult & 0.244 & 0.173 & 0.272 & 0.379 & 0.255 & 0.181 & 0.286 & 0.395 \\\\\n",
      "QMult & 0.843 & 0.785 & 0.885 & 0.941 & 0.806 & 0.742 & 0.849 & 0.918 \\\\\n",
      "QMult & 0.267 & 0.195 & 0.297 & 0.408 & 0.250 & 0.180 & 0.275 & 0.386 \\\\\n",
      "QMult & 0.270 & 0.198 & 0.299 & 0.410 & 0.254 & 0.181 & 0.282 & 0.396 \\\\\n",
      "TransE & 0.588 & 0.503 & 0.635 & 0.745 & 0.588 & 0.503 & 0.635 & 0.744 \\\\\n",
      "TransE & 0.221 & 0.144 & 0.240 & 0.373 & 0.220 & 0.143 & 0.239 & 0.374 \\\\\n",
      "TransE & 0.225 & 0.146 & 0.249 & 0.378 & 0.226 & 0.147 & 0.250 & 0.379 \\\\\n",
      "Pykeen_MuRE & 0.829 & 0.779 & 0.861 & 0.919 & 0.850 & 0.800 & 0.886 & 0.937 \\\\\n",
      "Pykeen_MuRE & 0.300 & 0.225 & 0.327 & 0.448 & 0.315 & 0.239 & 0.345 & 0.462 \\\\\n",
      "Pykeen_MuRE & 0.305 & 0.231 & 0.330 & 0.450 & 0.319 & 0.242 & 0.351 & 0.465 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp_dataset = get_kge_combined_table(dataset = \"YAGO15k\")\n",
    "latex_table = lp_dataset.to_latex(index=False, float_format=\"%.3f\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "Model & MRR & H@1 & H@3 & H@10 & MRR_Combined & H@1_Combined & H@3_Combined & H@10_Combined \\\\\n",
      "\\midrule\n",
      "ComplEx & 0.697 & 0.615 & 0.762 & 0.848 & 0.713 & 0.631 & 0.766 & 0.864 \\\\\n",
      "ComplEx & 0.255 & 0.199 & 0.279 & 0.364 & 0.256 & 0.198 & 0.277 & 0.366 \\\\\n",
      "ComplEx & 0.260 & 0.203 & 0.284 & 0.368 & 0.265 & 0.208 & 0.287 & 0.376 \\\\\n",
      "DeCaL & 0.738 & 0.658 & 0.795 & 0.886 & 0.707 & 0.628 & 0.749 & 0.872 \\\\\n",
      "DeCaL & 0.280 & 0.219 & 0.306 & 0.396 & 0.279 & 0.220 & 0.302 & 0.394 \\\\\n",
      "DeCaL & 0.281 & 0.221 & 0.308 & 0.394 & 0.282 & 0.223 & 0.305 & 0.395 \\\\\n",
      "DistMult & 0.709 & 0.613 & 0.779 & 0.862 & 0.718 & 0.630 & 0.781 & 0.864 \\\\\n",
      "DistMult & 0.243 & 0.185 & 0.267 & 0.347 & 0.242 & 0.184 & 0.265 & 0.349 \\\\\n",
      "DistMult & 0.246 & 0.186 & 0.270 & 0.356 & 0.247 & 0.188 & 0.270 & 0.360 \\\\\n",
      "Keci & 0.739 & 0.652 & 0.801 & 0.884 & 0.774 & 0.694 & 0.831 & 0.909 \\\\\n",
      "Keci & 0.277 & 0.210 & 0.308 & 0.402 & 0.290 & 0.226 & 0.317 & 0.409 \\\\\n",
      "Keci & 0.282 & 0.215 & 0.311 & 0.408 & 0.292 & 0.228 & 0.319 & 0.416 \\\\\n",
      "OMult & 0.763 & 0.703 & 0.798 & 0.875 & 0.775 & 0.716 & 0.811 & 0.882 \\\\\n",
      "OMult & 0.243 & 0.194 & 0.259 & 0.339 & 0.247 & 0.197 & 0.265 & 0.340 \\\\\n",
      "OMult & 0.242 & 0.191 & 0.261 & 0.337 & 0.249 & 0.198 & 0.268 & 0.347 \\\\\n",
      "QMult & 0.790 & 0.721 & 0.837 & 0.909 & 0.799 & 0.735 & 0.841 & 0.912 \\\\\n",
      "QMult & 0.252 & 0.200 & 0.272 & 0.354 & 0.249 & 0.198 & 0.266 & 0.348 \\\\\n",
      "QMult & 0.259 & 0.206 & 0.278 & 0.363 & 0.254 & 0.202 & 0.273 & 0.356 \\\\\n",
      "TransE & 0.597 & 0.496 & 0.666 & 0.773 & 0.599 & 0.500 & 0.667 & 0.774 \\\\\n",
      "TransE & 0.325 & 0.229 & 0.373 & 0.507 & 0.328 & 0.233 & 0.374 & 0.509 \\\\\n",
      "TransE & 0.330 & 0.232 & 0.380 & 0.515 & 0.333 & 0.238 & 0.383 & 0.515 \\\\\n",
      "Pykeen_MuRE & 0.790 & 0.733 & 0.827 & 0.892 & 0.784 & 0.726 & 0.821 & 0.890 \\\\\n",
      "Pykeen_MuRE & 0.312 & 0.251 & 0.334 & 0.428 & 0.311 & 0.250 & 0.336 & 0.426 \\\\\n",
      "Pykeen_MuRE & 0.315 & 0.255 & 0.339 & 0.431 & 0.312 & 0.250 & 0.337 & 0.430 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp_dataset = get_kge_combined_table(dataset = \"DB15K\")\n",
    "latex_table = lp_dataset.to_latex(index=False, float_format=\"%.3f\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "Model & MRR & H@1 & H@3 & H@10 & MRR_Combined & H@1_Combined & H@3_Combined & H@10_Combined \\\\\n",
      "\\midrule\n",
      "ComplEx & 0.458 & 0.352 & 0.512 & 0.662 & 0.422 & 0.312 & 0.477 & 0.635 \\\\\n",
      "ComplEx & 0.247 & 0.174 & 0.270 & 0.393 & 0.236 & 0.161 & 0.258 & 0.386 \\\\\n",
      "ComplEx & 0.247 & 0.174 & 0.269 & 0.393 & 0.236 & 0.162 & 0.258 & 0.386 \\\\\n",
      "DeCaL & 0.539 & 0.434 & 0.599 & 0.739 & 0.511 & 0.405 & 0.570 & 0.714 \\\\\n",
      "DeCaL & 0.271 & 0.193 & 0.295 & 0.424 & 0.265 & 0.188 & 0.287 & 0.420 \\\\\n",
      "DeCaL & 0.274 & 0.198 & 0.297 & 0.428 & 0.269 & 0.192 & 0.291 & 0.425 \\\\\n",
      "DistMult & 0.475 & 0.372 & 0.529 & 0.676 & 0.436 & 0.328 & 0.491 & 0.645 \\\\\n",
      "DistMult & 0.245 & 0.170 & 0.268 & 0.396 & 0.232 & 0.156 & 0.256 & 0.381 \\\\\n",
      "DistMult & 0.247 & 0.173 & 0.270 & 0.394 & 0.237 & 0.163 & 0.261 & 0.384 \\\\\n",
      "Keci & 0.543 & 0.438 & 0.602 & 0.744 & 0.496 & 0.388 & 0.554 & 0.704 \\\\\n",
      "Keci & 0.260 & 0.182 & 0.284 & 0.417 & 0.261 & 0.184 & 0.285 & 0.418 \\\\\n",
      "Keci & 0.264 & 0.186 & 0.289 & 0.420 & 0.262 & 0.186 & 0.284 & 0.417 \\\\\n",
      "OMult & 0.530 & 0.431 & 0.584 & 0.721 & 0.467 & 0.367 & 0.517 & 0.659 \\\\\n",
      "OMult & 0.230 & 0.156 & 0.253 & 0.376 & 0.227 & 0.156 & 0.247 & 0.367 \\\\\n",
      "OMult & 0.233 & 0.160 & 0.255 & 0.378 & 0.231 & 0.161 & 0.252 & 0.372 \\\\\n",
      "QMult & 0.544 & 0.443 & 0.598 & 0.736 & 0.516 & 0.410 & 0.573 & 0.717 \\\\\n",
      "QMult & 0.255 & 0.179 & 0.276 & 0.410 & 0.257 & 0.180 & 0.276 & 0.411 \\\\\n",
      "QMult & 0.261 & 0.184 & 0.283 & 0.413 & 0.258 & 0.183 & 0.279 & 0.409 \\\\\n",
      "TransE & 0.429 & 0.306 & 0.499 & 0.660 & 0.407 & 0.285 & 0.474 & 0.636 \\\\\n",
      "TransE & 0.307 & 0.217 & 0.341 & 0.481 & 0.300 & 0.212 & 0.332 & 0.474 \\\\\n",
      "TransE & 0.311 & 0.222 & 0.345 & 0.484 & 0.304 & 0.217 & 0.337 & 0.477 \\\\\n",
      "Pykeen_MuRE & 0.493 & 0.388 & 0.549 & 0.693 & 0.479 & 0.375 & 0.533 & 0.679 \\\\\n",
      "Pykeen_MuRE & 0.267 & 0.191 & 0.289 & 0.420 & 0.265 & 0.188 & 0.289 & 0.421 \\\\\n",
      "Pykeen_MuRE & 0.272 & 0.195 & 0.295 & 0.426 & 0.270 & 0.194 & 0.290 & 0.424 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp_dataset = get_kge_combined_table(dataset = \"FB15k-237\")\n",
    "latex_table = lp_dataset.to_latex(index=False, float_format=\"%.3f\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "relation & ComplEx & DeCaL & DistMult & Keci & OMult & Pykeen_MuRE & QMult & TransE \\\\\n",
      "\\midrule\n",
      "release\\_date & 6.763 & 8.853 & 10.286 & 6.767 & 6.027 & 8.118 & 5.441 & 7.736 \\\\\n",
      "loc.date\\_founded & 188.891 & 202.500 & 214.250 & 280.743 & 259.446 & 191.934 & 229.032 & 202.527 \\\\\n",
      "latitude & 9.068 & 9.745 & 8.100 & 11.793 & 8.770 & 6.898 & 8.749 & 8.231 \\\\\n",
      "longitude & 18.812 & 35.166 & 22.228 & 44.810 & 23.589 & 19.307 & 24.222 & 19.960 \\\\\n",
      "loc.area & 1862705.298 & 2140071.886 & 1969702.436 & 2506729.630 & 1848979.242 & 2039518.962 & 1661473.095 & 1652593.985 \\\\\n",
      "org.date\\_founded & 71.302 & 75.671 & 80.977 & 86.011 & 66.033 & 68.109 & 78.905 & 55.117 \\\\\n",
      "date\\_of\\_death & 29.374 & 43.568 & 31.622 & 51.183 & 19.055 & 28.132 & 23.275 & 18.517 \\\\\n",
      "date\\_of\\_birth & 19.568 & 38.989 & 19.883 & 51.098 & 18.621 & 19.678 & 17.941 & 15.649 \\\\\n",
      "height\\_meters & 0.082 & 0.075 & 0.079 & 0.074 & 0.074 & 0.074 & 0.080 & 0.077 \\\\\n",
      "weight\\_kg & 11.445 & 10.456 & 12.234 & 13.221 & 9.698 & 10.462 & 11.218 & 9.254 \\\\\n",
      "pop.\\_number & 10828477.724 & 9738142.918 & 12610408.458 & 24864733.548 & 6813235.175 & 11202151.809 & 6950680.292 & 8250794.618 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dataset = \"FB15k-237\"\n",
    "exp_dir = f\"Experiments/KGE_Combined/{dataset}_combined/\"\n",
    "df_average = pd.DataFrame() \n",
    "for sub_dir in os.listdir(exp_dir):\n",
    "    sub_path = os.path.join(exp_dir, sub_dir)\n",
    "    lit_results_path = os.path.join(sub_path,\"lit_results.json\" )\n",
    "    lit_results = pd.read_json(lit_results_path)\n",
    "    # Identify MAE and RMSE columns\n",
    "    mae_cols = [col for col in lit_results.columns if \"MAE\" in col]\n",
    "    # rmse_cols = [col for col in lit_results.columns if \"RMSE\" in col]\n",
    "\n",
    "    # Compute the mean across runs\n",
    "    lit_results[\"MAE\"] = lit_results[mae_cols].mean(axis=1)\n",
    "    # lit_results[\"RMSE\"] = lit_results[rmse_cols].mean(axis=1)\n",
    "\n",
    "    # Set relation as index\n",
    "    lit_results = lit_results.set_index(\"relation\")\n",
    "\n",
    "    lit_results = lit_results[[\"MAE\"]].rename(\n",
    "            columns={\"MAE\": f\"{sub_dir}\"}\n",
    "        )\n",
    "    # Merge with the main DataFrame\n",
    "    if df_average.empty:\n",
    "        df_average = lit_results\n",
    "    else:\n",
    "        df_average = df_average.join(lit_results, how=\"outer\")\n",
    "\n",
    "df_average = df_average.apply(pd.to_numeric).round(3)\n",
    "\n",
    "df_average = df_average[sorted(df_average.columns)]\n",
    "df_average = df_average.reset_index()\n",
    "df_average.loc[:, \"relation\"] = df_average.loc[:, \"relation\"].map(rel_map_fb15k)\n",
    "latex_table = df_average.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "relation & ComplEx & DeCaL & DistMult & Keci & OMult & Pykeen_MuRE & QMult & TransE \\\\\n",
      "\\midrule\n",
      "birthDate & 17.463 & 22.970 & 17.833 & 20.779 & 17.291 & 15.524 & 19.374 & 14.695 \\\\\n",
      "completionDate & 6.724 & 7.456 & 5.644 & 7.233 & 6.296 & 6.638 & 7.462 & 5.399 \\\\\n",
      "deathDate & 19.154 & 23.380 & 20.492 & 25.714 & 21.525 & 16.618 & 17.530 & 17.431 \\\\\n",
      "formationDate & 45.821 & 41.307 & 48.538 & 45.367 & 46.744 & 35.316 & 45.294 & 50.483 \\\\\n",
      "foundingDate & 57.343 & 61.638 & 53.991 & 69.287 & 54.075 & 56.183 & 59.783 & 50.741 \\\\\n",
      "height & 4.998 & 4.810 & 7.371 & 8.875 & 7.871 & 4.429 & 7.434 & 4.948 \\\\\n",
      "releaseDate & 10.998 & 9.714 & 9.452 & 10.061 & 13.678 & 8.331 & 12.210 & 7.848 \\\\\n",
      "latitude & 10.690 & 15.394 & 12.723 & 12.280 & 13.329 & 11.135 & 12.029 & 10.641 \\\\\n",
      "longitude & 32.111 & 58.214 & 35.658 & 52.848 & 40.605 & 36.108 & 43.241 & 29.852 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dataset = \"DB15K\"\n",
    "exp_dir = f\"Experiments/KGE_Combined/{dataset}_combined/\"\n",
    "df_average = pd.DataFrame() \n",
    "for sub_dir in os.listdir(exp_dir):\n",
    "    sub_path = os.path.join(exp_dir, sub_dir)\n",
    "    lit_results_path = os.path.join(sub_path,\"lit_results.json\" )\n",
    "    lit_results = pd.read_json(lit_results_path)\n",
    "    # Identify MAE and RMSE columns\n",
    "    mae_cols = [col for col in lit_results.columns if \"MAE\" in col]\n",
    "    # rmse_cols = [col for col in lit_results.columns if \"RMSE\" in col]\n",
    "\n",
    "    # Compute the mean across runs\n",
    "    lit_results[\"MAE\"] = lit_results[mae_cols].mean(axis=1)\n",
    "    # lit_results[\"RMSE\"] = lit_results[rmse_cols].mean(axis=1)\n",
    "\n",
    "    # Set relation as index\n",
    "    lit_results = lit_results.set_index(\"relation\")\n",
    "\n",
    "    lit_results = lit_results[[\"MAE\"]].rename(\n",
    "            columns={\"MAE\": f\"{sub_dir}\"}\n",
    "        )\n",
    "    # Merge with the main DataFrame\n",
    "    if df_average.empty:\n",
    "        df_average = lit_results\n",
    "    else:\n",
    "        df_average = df_average.join(lit_results, how=\"outer\")\n",
    "\n",
    "df_average = df_average.apply(pd.to_numeric).round(3)\n",
    "\n",
    "df_average = df_average[sorted(df_average.columns)]\n",
    "df_average = df_average.reset_index()\n",
    "df_average.loc[:, \"relation\"] = df_average.loc[:, \"relation\"].map(rel_map_db15k)\n",
    "latex_table = df_average.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "relation & ComplEx & DeCaL & DistMult & Keci & OMult & Pykeen_MuRE & QMult & TransE \\\\\n",
      "\\midrule\n",
      "diedOnDate & 38.598 & 39.489 & 35.883 & 32.416 & 31.659 & 29.635 & 36.565 & 30.994 \\\\\n",
      "happenedOnDate & 55.643 & 55.183 & 55.495 & 48.606 & 50.270 & 49.383 & 50.132 & 66.969 \\\\\n",
      "Latitude & 6.171 & 8.521 & 6.939 & 8.596 & 8.935 & 5.391 & 6.819 & 4.709 \\\\\n",
      "Longitude & 37.206 & 35.428 & 34.422 & 32.777 & 31.810 & 25.319 & 32.060 & 19.843 \\\\\n",
      "BornOnDate & 22.527 & 28.075 & 24.776 & 26.685 & 25.440 & 17.910 & 24.167 & 16.637 \\\\\n",
      "CreatedOnDate & 84.135 & 95.398 & 92.460 & 94.298 & 92.774 & 75.699 & 86.892 & 82.892 \\\\\n",
      "DestroyedOnDate & 32.229 & 33.435 & 30.165 & 31.682 & 29.746 & 27.913 & 42.901 & 34.545 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dataset = \"YAGO15k\"\n",
    "exp_dir = f\"Experiments/KGE_Combined/{dataset}_combined/\"\n",
    "df_average = pd.DataFrame() \n",
    "for sub_dir in os.listdir(exp_dir):\n",
    "    sub_path = os.path.join(exp_dir, sub_dir)\n",
    "    lit_results_path = os.path.join(sub_path,\"lit_results.json\" )\n",
    "    lit_results = pd.read_json(lit_results_path)\n",
    "    # Identify MAE and RMSE columns\n",
    "    mae_cols = [col for col in lit_results.columns if \"MAE\" in col]\n",
    "    # rmse_cols = [col for col in lit_results.columns if \"RMSE\" in col]\n",
    "\n",
    "    # Compute the mean across runs\n",
    "    lit_results[\"MAE\"] = lit_results[mae_cols].mean(axis=1)\n",
    "    # lit_results[\"RMSE\"] = lit_results[rmse_cols].mean(axis=1)\n",
    "\n",
    "    # Set relation as index\n",
    "    lit_results = lit_results.set_index(\"relation\")\n",
    "\n",
    "    lit_results = lit_results[[\"MAE\"]].rename(\n",
    "            columns={\"MAE\": f\"{sub_dir}\"}\n",
    "        )\n",
    "    # Merge with the main DataFrame\n",
    "    if df_average.empty:\n",
    "        df_average = lit_results\n",
    "    else:\n",
    "        df_average = df_average.join(lit_results, how=\"outer\")\n",
    "\n",
    "df_average = df_average.apply(pd.to_numeric).round(3)\n",
    "\n",
    "df_average = df_average[sorted(df_average.columns)]\n",
    "df_average = df_average.reset_index()\n",
    "df_average.loc[:, \"relation\"] = df_average.loc[:, \"relation\"].map(rel_map_yago15k)\n",
    "latex_table = df_average.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dataset = \"mutagenesis\"\n",
    "exp_dir = f\"Experiments/KGE_Combined/{dataset}_combined/\"\n",
    "df_average = pd.DataFrame() \n",
    "for sub_dir in os.listdir(exp_dir):\n",
    "    sub_path = os.path.join(exp_dir, sub_dir)\n",
    "    lit_results_path = os.path.join(sub_path,\"lit_results.json\" )\n",
    "    lit_results = pd.read_json(lit_results_path)\n",
    "    # Identify MAE and RMSE columns\n",
    "    mae_cols = [col for col in lit_results.columns if \"MAE\" in col]\n",
    "    # rmse_cols = [col for col in lit_results.columns if \"RMSE\" in col]\n",
    "\n",
    "    # Compute the mean across runs\n",
    "    lit_results[\"MAE\"] = lit_results[mae_cols].mean(axis=1)\n",
    "    # lit_results[\"RMSE\"] = lit_results[rmse_cols].mean(axis=1)\n",
    "\n",
    "    # Set relation as index\n",
    "    lit_results = lit_results.set_index(\"relation\")\n",
    "\n",
    "    lit_results = lit_results[[\"MAE\"]].rename(\n",
    "            columns={\"MAE\": f\"{sub_dir}\"}\n",
    "        )\n",
    "    # Merge with the main DataFrame\n",
    "    if df_average.empty:\n",
    "        df_average = lit_results\n",
    "    else:\n",
    "        df_average = df_average.join(lit_results, how=\"outer\")\n",
    "\n",
    "df_average = df_average.apply(pd.to_numeric).round(3)\n",
    "\n",
    "df_average = df_average[sorted(df_average.columns)]\n",
    "df_average = df_average.reset_index()\n",
    "latex_table = df_average.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "relation & MAE_80 & RMSE_80 & MAE_60 & RMSE_60 & MAE_40 & RMSE_40 & MAE_20 & RMSE_20 \\\\\n",
      "\\midrule\n",
      "release\\_date & 3.80 & 2.10 & 6.72 & 7.28 & 7.76 & 8.63 & 18.56 & 15.09 \\\\\n",
      "loc.date\\_founded & 6.00 & 1.47 & 8.94 & 1.97 & 10.59 & 3.17 & 14.59 & 3.24 \\\\\n",
      "latitude & -1.63 & -2.99 & 1.44 & -2.23 & 7.62 & 3.93 & 16.90 & 11.14 \\\\\n",
      "longitude & -5.16 & 2.21 & 2.16 & 2.47 & 8.52 & 6.00 & 19.01 & 12.13 \\\\\n",
      "loc.area & 0.26 & 0.12 & -2.38 & 0.06 & -4.59 & 0.17 & -3.94 & 0.12 \\\\\n",
      "org.date\\_founded & 6.25 & 6.62 & 6.52 & 7.86 & 5.07 & 3.61 & 18.65 & 12.21 \\\\\n",
      "date\\_of\\_death & 7.27 & -3.69 & 3.15 & 0.98 & 2.25 & -0.10 & 25.96 & 1.36 \\\\\n",
      "date\\_of\\_birth & 1.71 & -1.04 & 3.06 & 0.72 & 1.79 & -3.78 & 10.79 & 5.22 \\\\\n",
      "height\\_meters & 1.60 & 0.95 & -0.64 & -0.92 & 4.08 & 4.35 & 7.31 & 8.13 \\\\\n",
      "weight\\_kg & 4.30 & 2.76 & 4.11 & 4.45 & -3.53 & -0.62 & 5.72 & 13.46 \\\\\n",
      "pop.\\_number & 20.54 & -1.58 & 46.81 & 4.89 & -33.03 & 4.29 & -28.26 & 3.26 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of dataset fractions you want to process\n",
    "fractions = [100, 80, 60, 40, 20]\n",
    "\n",
    "# Base path with a placeholder for the fraction\n",
    "base_path = \"Experiments/Ablations/FB15k-237_{}/TransE/lit_results.csv\"\n",
    "\n",
    "\n",
    "# Initialize the final DataFrame\n",
    "final_df = None\n",
    "\n",
    "for frac in fractions:\n",
    "    file_path = base_path.format(str(frac))\n",
    "    try:\n",
    "        # Read the current CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename MAE and RMSE columns with run-specific suffix\n",
    "        df = df.rename(columns={\n",
    "            \"MAE\": f\"MAE_{frac}\",\n",
    "            \"RMSE\": f\"RMSE_{frac}\"\n",
    "        })\n",
    "\n",
    "        # If it's the first run, start the final DataFrame\n",
    "        if final_df is None:\n",
    "            final_df = df\n",
    "        else:\n",
    "            # Merge on 'relation' to align rows\n",
    "            final_df = pd.merge(final_df, df, on=\"relation\", how=\"outer\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for fraction {frac}\")\n",
    "\n",
    "# Save or inspect the final DataFrame\n",
    "# final_df.to_csv(\"merged_metrics.csv\", index=False)\n",
    "# filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "\n",
    "filtered_df = final_df.copy(deep=True)\n",
    "filtered_df.loc[:, \"relation\"] = filtered_df.loc[:, \"relation\"].map(rel_map_fb15k)\n",
    "# latex_table = filtered_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "# print(latex_table)\n",
    "# Step 2: Compute relative changes (as percentages) for both MAE and RMSE columns\n",
    "for metric in ['MAE', 'RMSE']:\n",
    "    for col in [f'{metric}_80', f'{metric}_60', f'{metric}_40', f'{metric}_20']:\n",
    "        # Compute relative change for each MAE and RMSE column\n",
    "        filtered_df.loc[:, col] = (((filtered_df[col] - filtered_df[f'{metric}_100']) / filtered_df[f'{metric}_100']) * 100).round(2)\n",
    "\n",
    "# Step 3: Drop the *_100 columns if not needed\n",
    "filtered_df = filtered_df.drop(columns=[f'MAE_100', f'RMSE_100'])\n",
    "latex_table = filtered_df.to_latex(index=False, float_format=\"%.2f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "relation & MAE_80 & RMSE_80 & MAE_60 & RMSE_60 & MAE_40 & RMSE_40 & MAE_20 & RMSE_20 \\\\\n",
      "\\midrule\n",
      "diedOnDate & -0.96 & -3.44 & -1.25 & -7.97 & -0.79 & -11.51 & 2.46 & -13.20 \\\\\n",
      "happenedOnDate & -0.15 & -2.75 & 6.05 & 3.67 & 2.37 & -4.86 & 37.78 & 32.69 \\\\\n",
      "Latitude & 6.43 & 6.38 & 2.12 & -2.17 & 4.58 & -3.02 & 14.60 & 2.92 \\\\\n",
      "Longitude & 10.05 & 5.73 & 6.99 & 4.41 & 8.37 & 10.35 & 28.60 & 26.69 \\\\\n",
      "BornOnDate & 11.48 & -2.72 & 5.73 & -6.04 & 3.95 & -4.23 & 13.22 & 1.87 \\\\\n",
      "CreatedOnDate & 4.65 & 0.96 & 5.68 & 1.13 & 4.08 & -1.16 & 13.86 & -1.74 \\\\\n",
      "DestroyedOnDate & 10.14 & -0.14 & 10.39 & 0.83 & 30.34 & 10.15 & 44.55 & 21.48 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of dataset fractions you want to process\n",
    "fractions = [100, 80, 60, 40, 20]\n",
    "\n",
    "# Base path with a placeholder for the fraction\n",
    "base_path = \"Experiments/Ablations/YAGO15k_{}/TransE/lit_results.csv\"\n",
    "\n",
    "\n",
    "# Initialize the final DataFrame\n",
    "final_df = None\n",
    "\n",
    "for frac in fractions:\n",
    "    file_path = base_path.format(str(frac))\n",
    "    try:\n",
    "        # Read the current CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename MAE and RMSE columns with run-specific suffix\n",
    "        df = df.rename(columns={\n",
    "            \"MAE\": f\"MAE_{frac}\",\n",
    "            \"RMSE\": f\"RMSE_{frac}\"\n",
    "        })\n",
    "\n",
    "        # If it's the first run, start the final DataFrame\n",
    "        if final_df is None:\n",
    "            final_df = df\n",
    "        else:\n",
    "            # Merge on 'relation' to align rows\n",
    "            final_df = pd.merge(final_df, df, on=\"relation\", how=\"outer\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for fraction {frac}\")\n",
    "\n",
    "# Save or inspect the final DataFrame\n",
    "# final_df.to_csv(\"merged_metrics.csv\", index=False)\n",
    "# filtered_df = final_df.loc[:, final_df.columns.str.contains('relation|MAE', case=False)]\n",
    "\n",
    "filtered_df = final_df.copy(deep=True)\n",
    "filtered_df.loc[:, \"relation\"] = filtered_df.loc[:, \"relation\"].map(rel_map_yago15k)\n",
    "# latex_table = filtered_df.to_latex(index=False, float_format=\"%.3f\")  # Format floats to 3 decimal places\n",
    "# print(latex_table)\n",
    "# Step 2: Compute relative changes (as percentages) for both MAE and RMSE columns\n",
    "for metric in ['MAE', 'RMSE']:\n",
    "    for col in [f'{metric}_80', f'{metric}_60', f'{metric}_40', f'{metric}_20']:\n",
    "        # Compute relative change for each MAE and RMSE column\n",
    "        filtered_df.loc[:, col] = (((filtered_df[col] - filtered_df[f'{metric}_100']) / filtered_df[f'{metric}_100']) * 100).round(2)\n",
    "\n",
    "# Step 3: Drop the *_100 columns if not needed\n",
    "filtered_df = filtered_df.drop(columns=[f'MAE_100', f'RMSE_100'])\n",
    "latex_table = filtered_df.to_latex(index=False, float_format=\"%.2f\")  # Format floats to 3 decimal places\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
